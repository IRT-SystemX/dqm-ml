{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric notebook test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook show how to use domain_gap metrics with a notebook after installing DataQualityMetric (DQM) library. Those metrics aim to evaluate the gap between two datasets which would allow to estimated required finetuning while adapting the model, be used as a loss function in generative models or rate simulated data similarity with real life data. For now, metrics implementation are only image type compatible.\n",
    "\n",
    "The computation of the metrics requires a configuration file in which all the parameters for the data processing, feature extractor model and method parameters are defined. Examples are available in dqm/domain_gap/cfg/{metric_name} folder. \n",
    "\n",
    "We authorize homemade models for the computation of features, however those model must be pytorch friendly and contain both architecture and weights in a single \".pt\" file. Default model are retrieved from torch hub models with imagenet dataset pretrained weight.\n",
    "\n",
    "Here is a list of the metrics:\n",
    "- FID\n",
    "- Wasserstein\n",
    "- PAD\n",
    "- KLMVN\n",
    "\n",
    "!!! pay attention that the preprocessing applied to the images (parameters of the \"DATA\" part in the configuration file) is compatible with the model inputs, it may be necessary to check training prepocessor pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FID: Frechet Inception Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "metric name: fid\n",
      "source folder: ./synthetic_source_images (100 images)\n",
      "target folder: ./synthetic_target_images (100 images)\n",
      "Preprocessing:\n",
      "    -image resize: (299,299)\n",
      "    -image normalize:\n",
      "        -mean: [0.485, 0.456, 0.406]\n",
      "        -std: [0.229, 0.224, 0.225]\n",
      "device: cpu\n",
      "feature extraction layer: -2\n",
      "model architecture : InceptionV3 (default)\n",
      "fid score: 3.927738982265243\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from dqm.domain_gap.metrics import FID\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# don't show user warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Instanciate the metric class\n",
    "fid = FID()\n",
    "\n",
    "# Generate synthetic image dataset folders\n",
    "def generate_image_dataset(num_images, height, width, folder_name):\n",
    "    \"\"\"\n",
    "    Generate a set of random images saved to a specified folder.\n",
    "    Each image will have 3 channels (RGB) with pixel values in the range [0, 255].\n",
    "    \"\"\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Create random image data with RGB values in the range [0, 255]\n",
    "        img_array = torch.randint(0, 256, (height, width, 3), dtype=torch.uint8).numpy()\n",
    "        img = Image.fromarray(img_array)\n",
    "        \n",
    "        # Save image to folder\n",
    "        img.save(os.path.join(folder_name, f\"img_{i:04d}.png\"))\n",
    "\n",
    "# Paths to synthetic image folders\n",
    "source_folder = \"./synthetic_source_images\"\n",
    "target_folder = \"./synthetic_target_images\"\n",
    "\n",
    "# Generate synthetic datasets\n",
    "generate_image_dataset(100, 299, 299, source_folder)\n",
    "generate_image_dataset(100, 299, 299, target_folder)\n",
    "\n",
    "# Define your own config file, you can find examples in dqm/domain_gap/cfg/{metric_name}\n",
    "fid_config_json = {\n",
    "\t\"DATA\": {\n",
    "\t\t\"batch_size\": 32,                      # Features will be compute on {batch_size} images at the same time\n",
    "\t\t\"height\": 299,                         # Resize images height to {height} value\n",
    "\t\t\"width\": 299,                          # Resize images width to {width} value\n",
    "\t\t\"norm_mean\": [                         # Normalize images mean with {norm_mean} values for RGB channels\n",
    "\t\t\t\t0.485,\n",
    "\t\t\t\t0.456,\n",
    "\t\t\t\t0.406\n",
    "\t\t\t],\n",
    "\t\t\"norm_std\": [                          # Normalize images std with {norm_std} values for RGB cahnnels\n",
    "\t\t\t\t0.229,\n",
    "\t\t\t\t0.224,\n",
    "\t\t\t\t0.225\n",
    "\t\t\t],\n",
    "\t\t\"source\": source_folder,      # source images are retrieved from {source} path\n",
    "\t\t\"target\": target_folder       # target images are retrieved from {target} path\n",
    "\t},\n",
    "\t\"MODEL\": {\n",
    "\t\t\"device\": \"cpu\",                       # Metric will be computed in {device}\n",
    "\t\t\"n_layer_feature\": -2                  # the layer extractor feature will be the:\n",
    "    \t},                                     # i-th if int       |  {n_layer_feature} if str\n",
    "\t\"METHOD\": {\n",
    "\t\t\"name\": \"fid\"                          # Metric name, used only with CLI\n",
    "\t}\n",
    "}\n",
    "\n",
    "# Compute the metric\n",
    "dist = fid.compute_image_distance(fid_config_json)\n",
    "print(\"-\"*80)\n",
    "print(f\"metric name: {fid_config_json['METHOD'][\"name\"]}\")\n",
    "print(f\"source folder: {source_folder} ({len(os.listdir(source_folder))} images)\")\n",
    "print(f\"target folder: {target_folder} ({len(os.listdir(target_folder))} images)\")\n",
    "print(\"Preprocessing:\")\n",
    "print(\" \"*4+f\"-image resize: ({fid_config_json[\"DATA\"][\"width\"]},{fid_config_json[\"DATA\"][\"height\"]})\")\n",
    "print(\" \"*4+\"-image normalize:\")\n",
    "print(\" \"*8+f\"-mean: {fid_config_json[\"DATA\"][\"norm_mean\"]}\")\n",
    "print(\" \"*8+f\"-std: {fid_config_json[\"DATA\"][\"norm_std\"]}\")\n",
    "print(f\"device: {fid_config_json[\"MODEL\"][\"device\"]}\")\n",
    "print(f\"feature extraction layer: {fid_config_json[\"MODEL\"][\"n_layer_feature\"]}\")\n",
    "if \"arch\" not in fid_config_json[\"MODEL\"].keys():\n",
    "\tprint(\"(default) model architecture : InceptionV3\")\n",
    "print(f\"fid score: {dist.item()}\")\n",
    "print(\"-\"*80)\n",
    "# remove generated source and target folder\n",
    "!rm -r synthetic_source_images/ synthetic_target_images/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wasserstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "metric name: wasserstein\n",
      "source folder: ./synthetic_source_images (100 images)\n",
      "target folder: ./synthetic_target_images (100 images)\n",
      "Preprocessing:\n",
      "    -image resize: (299,299)\n",
      "    -image normalize:\n",
      "        -mean: [0.485, 0.456, 0.406]\n",
      "        -std: [0.229, 0.224, 0.225]\n",
      "device: cpu\n",
      "feature extraction layer: -2\n",
      "model architecture : InceptionV3 (default)\n",
      "wasserstein score: 0.012761476961261994\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from dqm.domain_gap.metrics import Wasserstein\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Instanciate the metric class\n",
    "wass = Wasserstein()\n",
    "\n",
    "# Generate synthetic image dataset folders\n",
    "def generate_image_dataset(num_images, height, width, folder_name):\n",
    "    \"\"\"\n",
    "    Generate a set of random images saved to a specified folder.\n",
    "    Each image will have 3 channels (RGB) with pixel values in the range [0, 255].\n",
    "    \"\"\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Create random image data with RGB values in the range [0, 255]\n",
    "        img_array = torch.randint(0, 256, (height, width, 3), dtype=torch.uint8).numpy()\n",
    "        img = Image.fromarray(img_array)\n",
    "        \n",
    "        # Save image to folder\n",
    "        img.save(os.path.join(folder_name, f\"img_{i:04d}.png\"))\n",
    "\n",
    "# Paths to synthetic image folders\n",
    "source_folder = \"./synthetic_source_images\"\n",
    "target_folder = \"./synthetic_target_images\"\n",
    "\n",
    "# Generate synthetic datasets\n",
    "generate_image_dataset(100, 299, 299, source_folder)\n",
    "generate_image_dataset(100, 299, 299, target_folder)\n",
    "\n",
    "# Define your own config file, you can find examples in dqm/domain_gap/cfg/{metric_name}\n",
    "wass_config_json = {\n",
    "\t\"DATA\": {\n",
    "\t\t\"batch_size\": 10,\n",
    "\t\t\"height\": 299,\n",
    "\t\t\"width\": 299,\n",
    "\t\t\"norm_mean\": [\n",
    "\t\t\t\t0.485,\n",
    "\t\t\t\t0.456,\n",
    "\t\t\t\t0.406\n",
    "\t\t\t],\n",
    "\t\t\"norm_std\": [\n",
    "\t\t\t\t0.229,\n",
    "\t\t\t\t0.224,\n",
    "\t\t\t\t0.225\n",
    "\t\t\t],\n",
    "\t\t\"source\": source_folder, \n",
    "\t\t\"target\": target_folder  \n",
    "\t},\n",
    "\t\"MODEL\": {\n",
    "        \"arch\": \"resnet18\",\n",
    "\t\t\"device\": \"cpu\",\n",
    "\t\t\"n_layer_feature\": -2\n",
    "    \t},\n",
    "\t\"METHOD\": {\n",
    "\t\t\"name\": \"wasserstein\",\n",
    "\t\t\"dimension\": \"1D\"\n",
    "\t}\n",
    "}\n",
    "\n",
    "# Compute the metric\n",
    "dist = wass.compute_1D_distance(wass_config_json)\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(f\"metric name: {wass_config_json['METHOD'][\"name\"]}\")\n",
    "print(f\"source folder: {source_folder} ({len(os.listdir(source_folder))} images)\")\n",
    "print(f\"target folder: {target_folder} ({len(os.listdir(target_folder))} images)\")\n",
    "print(\"Preprocessing:\")\n",
    "print(\" \"*4+f\"-image resize: ({fid_config_json[\"DATA\"][\"width\"]},{fid_config_json[\"DATA\"][\"height\"]})\")\n",
    "print(\" \"*4+\"-image normalize:\")\n",
    "print(\" \"*8+f\"-mean: {fid_config_json[\"DATA\"][\"norm_mean\"]}\")\n",
    "print(\" \"*8+f\"-std: {fid_config_json[\"DATA\"][\"norm_std\"]}\")\n",
    "print(f\"device: {fid_config_json[\"MODEL\"][\"device\"]}\")\n",
    "print(f\"feature extraction layer: {fid_config_json[\"MODEL\"][\"n_layer_feature\"]}\")\n",
    "if \"arch\" not in fid_config_json[\"MODEL\"].keys():\n",
    "\tprint(\"model architecture : InceptionV3 (default)\")\n",
    "print(f\"wasserstein score: {dist.item()}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# remove generated source and target folder\n",
    "!rm -r synthetic_source_images/ synthetic_target_images/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KLMVN: Kullback-Leibler for MultiVariate Normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "metric name: klmvn\n",
      "source folder: ./synthetic_source_images (100 images)\n",
      "target folder: ./synthetic_target_images (100 images)\n",
      "Preprocessing:\n",
      "    -image resize: (299,299)\n",
      "    -image normalize:\n",
      "        -mean: [0.485, 0.456, 0.406]\n",
      "        -std: [0.229, 0.224, 0.225]\n",
      "device: cpu\n",
      "feature extraction layer: -2\n",
      "model architecture : InceptionV3 (default)\n",
      "kullback-Leibler score: 290.20548276043786\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from dqm.domain_gap.metrics import KLMVN\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Instanciate the metric class\n",
    "klmvn = KLMVN()\n",
    "\n",
    "# Generate synthetic image dataset folders\n",
    "def generate_image_dataset(num_images, height, width, folder_name):\n",
    "    \"\"\"\n",
    "    Generate a set of random images saved to a specified folder.\n",
    "    Each image will have 3 channels (RGB) with pixel values in the range [0, 255].\n",
    "    \"\"\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Create random image data with RGB values in the range [0, 255]\n",
    "        img_array = torch.randint(0, 256, (height, width, 3), dtype=torch.uint8).numpy()\n",
    "        img = Image.fromarray(img_array)\n",
    "        \n",
    "        # Save image to folder\n",
    "        img.save(os.path.join(folder_name, f\"img_{i:04d}.png\"))\n",
    "\n",
    "# Paths to synthetic image folders\n",
    "source_folder = \"./synthetic_source_images\"\n",
    "target_folder = \"./synthetic_target_images\"\n",
    "\n",
    "# Generate synthetic datasets\n",
    "generate_image_dataset(100, 299, 299, source_folder)\n",
    "generate_image_dataset(100, 299, 299, target_folder)\n",
    "\n",
    "# Define your own config file, you can find examples in dqm/domain_gap/cfg/{metric_name}\n",
    "klmvn_config_json = {\n",
    "\t\"DATA\": {\n",
    "\t\t\"batch_size\": 10,\n",
    "\t\t\"height\": 28,\n",
    "\t\t\"width\": 28,\n",
    "\t\t\"norm_mean\": [\n",
    "\t\t\t\t0.485,\n",
    "\t\t\t\t0.456,\n",
    "\t\t\t\t0.406\n",
    "\t\t\t],\n",
    "\t\t\"norm_std\": [\n",
    "\t\t\t\t0.229,\n",
    "\t\t\t\t0.224,\n",
    "\t\t\t\t0.225\n",
    "\t\t\t],\n",
    "\t\t\"source\": source_folder, \n",
    "\t\t\"target\": target_folder \n",
    "\t},\n",
    "\t\"MODEL\": {\n",
    "        \"arch\": \"resnet18\",\n",
    "\t\t\"device\": \"cpu\",\n",
    "\t\t\"n_layer_feature\": -2\n",
    "    \t},\n",
    "\t\"METHOD\": {\n",
    "\t\t\"name\": \"klmvn\"\n",
    "\t}\n",
    "}\n",
    "\n",
    "# Compute the metric\n",
    "dist = klmvn.compute_image_distance(klmvn_config_json)\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(f\"metric name: {klmvn_config_json['METHOD'][\"name\"]}\")\n",
    "print(f\"source folder: {source_folder} ({len(os.listdir(source_folder))} images)\")\n",
    "print(f\"target folder: {target_folder} ({len(os.listdir(target_folder))} images)\")\n",
    "print(\"Preprocessing:\")\n",
    "print(\" \"*4+f\"-image resize: ({fid_config_json[\"DATA\"][\"width\"]},{fid_config_json[\"DATA\"][\"height\"]})\")\n",
    "print(\" \"*4+\"-image normalize:\")\n",
    "print(\" \"*8+f\"-mean: {fid_config_json[\"DATA\"][\"norm_mean\"]}\")\n",
    "print(\" \"*8+f\"-std: {fid_config_json[\"DATA\"][\"norm_std\"]}\")\n",
    "print(f\"device: {fid_config_json[\"MODEL\"][\"device\"]}\")\n",
    "print(f\"feature extraction layer: {fid_config_json[\"MODEL\"][\"n_layer_feature\"]}\")\n",
    "if \"arch\" not in fid_config_json[\"MODEL\"].keys():\n",
    "\tprint(\"model architecture : InceptionV3 (default)\")\n",
    "print(f\"kullback-Leibler score: {dist.item()}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# remove generated source and target folder\n",
    "!rm -r synthetic_source_images/ synthetic_target_images/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAD: Proxy A Distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "metric name: proxy\n",
      "source folder: ./synthetic_source_images (100 images)\n",
      "target folder: ./synthetic_target_images (100 images)\n",
      "Preprocessing:\n",
      "    -image resize: (224,224)\n",
      "    -image normalize:\n",
      "        -mean: [0.485, 0.456, 0.406]\n",
      "        -std: [0.229, 0.224, 0.225]\n",
      "device: cpu\n",
      "model architecture: ['efficientnet_b0', 'vgg16']\n",
      "feature extraction layer: -2\n",
      "Proxy A Distance score: 0.9131710737349599\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from dqm.domain_gap.metrics import ProxyADistance\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Instanciate the metric class\n",
    "pad = ProxyADistance()\n",
    "\n",
    "# Generate synthetic image dataset folders\n",
    "def generate_image_dataset(num_images, height, width, folder_name):\n",
    "    \"\"\"\n",
    "    Generate a set of random images saved to a specified folder.\n",
    "    Each image will have 3 channels (RGB) with pixel values in the range [0, 255].\n",
    "    \"\"\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Create random image data with RGB values in the range [0, 255]\n",
    "        img_array = torch.randint(0, 256, (height, width, 3), dtype=torch.uint8).numpy()\n",
    "        img = Image.fromarray(img_array)\n",
    "        \n",
    "        # Save image to folder\n",
    "        img.save(os.path.join(folder_name, f\"img_{i:04d}.png\"))\n",
    "\n",
    "# Paths to synthetic image folders\n",
    "source_folder = \"./synthetic_source_images\"\n",
    "target_folder = \"./synthetic_target_images\"\n",
    "\n",
    "# Generate synthetic datasets\n",
    "generate_image_dataset(100, 299, 299, source_folder)\n",
    "generate_image_dataset(100, 299, 299, target_folder)\n",
    "\n",
    "# Define your own config file, you can find examples in dqm/domain_gap/cfg/{metric_name}\n",
    "pad_config_json = {\n",
    "\t\"DATA\": {\n",
    "\t\t\"height\": 224,\n",
    "\t\t\"width\": 224,\n",
    "\t\t\"batch_size\": 10,\n",
    "\t\t\"norm_mean\": [\n",
    "\t\t\t0.485,\n",
    "\t\t\t0.456,\n",
    "\t\t\t0.406\n",
    "\t\t],\n",
    "\t\t\"norm_std\": [\n",
    "\t\t\t0.229,\n",
    "\t\t\t0.224,\n",
    "\t\t\t0.225\n",
    "\t\t],\n",
    "\t\t\"source\": source_folder, \n",
    "\t\t\"target\": target_folder \n",
    "\t},\n",
    "\t\"MODEL\": {\n",
    "\t\t\"arch\": [\"efficientnet_b0\",\"vgg16\"],\n",
    "\t\t\"device\": \"cpu\",\n",
    "\t\t\"n_layer_feature\": -2\n",
    "\t},\n",
    "\t\"METHOD\": {\n",
    "\t\t\"name\": \"proxy\",\n",
    "        \"evaluator\": \"mse\"\n",
    "\t}\n",
    "}\n",
    "\n",
    "\n",
    "# Compute the metric\n",
    "dist = pad.compute_image_distance(pad_config_json)\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(f\"metric name: {pad_config_json['METHOD'][\"name\"]}\")\n",
    "print(f\"source folder: {source_folder} ({len(os.listdir(source_folder))} images)\")\n",
    "print(f\"target folder: {target_folder} ({len(os.listdir(target_folder))} images)\")\n",
    "print(\"Preprocessing:\")\n",
    "print(\" \"*4+f\"-image resize: ({pad_config_json[\"DATA\"][\"width\"]},{pad_config_json[\"DATA\"][\"height\"]})\")\n",
    "print(\" \"*4+\"-image normalize:\")\n",
    "print(\" \"*8+f\"-mean: {pad_config_json[\"DATA\"][\"norm_mean\"]}\")\n",
    "print(\" \"*8+f\"-std: {pad_config_json[\"DATA\"][\"norm_std\"]}\")\n",
    "print(f\"device: {pad_config_json[\"MODEL\"][\"device\"]}\")\n",
    "print(f\"model architecture: {pad_config_json[\"MODEL\"][\"arch\"]}\")\n",
    "print(f\"feature extraction layer: {pad_config_json[\"MODEL\"][\"n_layer_feature\"]}\")\n",
    "print(f\"Proxy A Distance score: {dist.item()}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# remove generated source and target folder\n",
    "!rm -r synthetic_source_images/ synthetic_target_images/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMD: Maximum Mean Discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoann/data_quality_metrics/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/yoann/data_quality_metrics/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same features\n",
      "torch.Size([100, 512])\n",
      "--------------------------------------------------------------------------------\n",
      "metric name: mmd\n",
      "source folder: /home/yoann/data/synthetic_source_images (100 images)\n",
      "target folder: /home/yoann/data/synthetic_target_images (100 images)\n",
      "Preprocessing:\n",
      "    -image resize: (224,224)\n",
      "    -image normalize:\n",
      "        -mean: [0.485, 0.456, 0.406]\n",
      "        -std: [0.229, 0.224, 0.225]\n",
      "device: cpu\n",
      "model architecture: resnet18\n",
      "feature extraction layer: -2\n",
      "Maximum Mean Discrepancy score: 0.05866938456892967\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from dqm.domain_gap.metrics import MMD\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Instanciate the metric class\n",
    "mmd = MMD()\n",
    "\n",
    "# Generate synthetic image dataset folders\n",
    "def generate_image_dataset(num_images, height, width, folder_name):\n",
    "    \"\"\"\n",
    "    Generate a set of random images saved to a specified folder.\n",
    "    Each image will have 3 channels (RGB) with pixel values in the range [0, 255].\n",
    "    \"\"\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Create random image data with RGB values in the range [0, 255]\n",
    "        img_array = torch.randint(0, 256, (height, width, 3), dtype=torch.uint8).numpy()\n",
    "        img = Image.fromarray(img_array)\n",
    "        \n",
    "        # Save image to folder\n",
    "        img.save(os.path.join(folder_name, f\"img_{i:04d}.png\"))\n",
    "\n",
    "# Paths to synthetic image folders\n",
    "source_folder = \"/home/yoann/data/synthetic_source_images\"\n",
    "target_folder = \"/home/yoann/data/synthetic_target_images\"\n",
    "\n",
    "# Generate synthetic datasets\n",
    "generate_image_dataset(100, 299, 299, source_folder)\n",
    "generate_image_dataset(100, 299, 299, target_folder)\n",
    "\n",
    "# Define your own config file, you can find examples in dqm/domain_gap/cfg/{metric_name}\n",
    "mmd_config_json = {\n",
    "\t\"DATA\": {\n",
    "\t\t\"height\": 224,\n",
    "\t\t\"width\": 224,\n",
    "\t\t\"batch_size\": 10,\n",
    "\t\t\"norm_mean\": [\n",
    "\t\t\t0.485,\n",
    "\t\t\t0.456,\n",
    "\t\t\t0.406\n",
    "\t\t],\n",
    "\t\t\"norm_std\": [\n",
    "\t\t\t0.229,\n",
    "\t\t\t0.224,\n",
    "\t\t\t0.225\n",
    "\t\t],\n",
    "\t\t\"source\": source_folder, \n",
    "\t\t\"target\": target_folder \n",
    "\t},\n",
    "\t\"MODEL\": {\n",
    "        \"arch\": \"resnet18\",\n",
    "\t\t\"device\": \"cpu\",\n",
    "\t\t\"n_layer_feature\": -2\n",
    "    \t},\n",
    "\t\"METHOD\": {\n",
    "\t\t\"name\": \"mmd\",\n",
    "\t\t\"kernel\": \"linear\",\n",
    "\t\t\"kernel_params\": {\n",
    "\t\t\t\"gamma\": 1.0,\n",
    "\t\t\t\"degree\": 3.0,\n",
    "\t\t\t\"coefficient0\": 1.0 \n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "\n",
    "\n",
    "# Compute the metric\n",
    "dist = mmd.compute(mmd_config_json)\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(f\"metric name: {mmd_config_json['METHOD'][\"name\"]}\")\n",
    "print(f\"source folder: {source_folder} ({len(os.listdir(source_folder))} images)\")\n",
    "print(f\"target folder: {target_folder} ({len(os.listdir(target_folder))} images)\")\n",
    "print(\"Preprocessing:\")\n",
    "print(\" \"*4+f\"-image resize: ({mmd_config_json[\"DATA\"][\"width\"]},{mmd_config_json[\"DATA\"][\"height\"]})\")\n",
    "print(\" \"*4+\"-image normalize:\")\n",
    "print(\" \"*8+f\"-mean: {mmd_config_json[\"DATA\"][\"norm_mean\"]}\")\n",
    "print(\" \"*8+f\"-std: {mmd_config_json[\"DATA\"][\"norm_std\"]}\")\n",
    "print(f\"device: {mmd_config_json[\"MODEL\"][\"device\"]}\")\n",
    "print(f\"model architecture: {mmd_config_json[\"MODEL\"][\"arch\"]}\")\n",
    "print(f\"feature extraction layer: {mmd_config_json[\"MODEL\"][\"n_layer_feature\"]}\")\n",
    "print(f\"Maximum Mean Discrepancy score: {dist}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# remove generated source and target folder\n",
    "# !rm -r synthetic_source_images/ synthetic_target_images/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMD: Central Moments Discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "metric name: cmd\n",
      "source folder: /home/yoann/data/synthetic_source_images_mini (10 images)\n",
      "target folder: /home/yoann/data/synthetic_target_images_mini (10 images)\n",
      "Preprocessing:\n",
      "    -image resize: (224,224)\n",
      "    -image normalize:\n",
      "        -mean: [0.485, 0.456, 0.406]\n",
      "        -std: [0.229, 0.224, 0.225]\n",
      "device: cpu\n",
      "model architecture: resnet18\n",
      "feature extraction layer: ['maxpool', 'layer1.1.relu_1', 'layer2.1.relu_1', 'layer3.1.relu_1', 'layer4.1.relu_1']\n",
      "Central Moments Discrepancy score: 0.009210899472236633\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dqm.domain_gap.metrics import CMD\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Instanciate the metric class\n",
    "cmd = CMD()\n",
    "\n",
    "# Generate synthetic image dataset folders\n",
    "def generate_image_dataset(num_images, height, width, folder_name):\n",
    "    \"\"\"\n",
    "    Generate a set of random images saved to a specified folder.\n",
    "    Each image will have 3 channels (RGB) with pixel values in the range [0, 255].\n",
    "    \"\"\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Create random image data with RGB values in the range [0, 255]\n",
    "        img_array = torch.randint(0, 256, (height, width, 3), dtype=torch.uint8).numpy()\n",
    "        img = Image.fromarray(img_array)\n",
    "        \n",
    "        # Save image to folder\n",
    "        img.save(os.path.join(folder_name, f\"img_{i:04d}.png\"))\n",
    "\n",
    "# Paths to synthetic image folders\n",
    "source_folder = \"/home/yoann/data/synthetic_source_images_mini\"\n",
    "target_folder = \"/home/yoann/data/synthetic_target_images_mini\"\n",
    "\n",
    "# Generate synthetic datasets\n",
    "#generate_image_dataset(100, 299, 299, source_folder)\n",
    "#generate_image_dataset(100, 299, 299, target_folder)\n",
    "\n",
    "# Define your own config file, you can find examples in dqm/domain_gap/cfg/{metric_name}\n",
    "cmd_config_json = {\n",
    "\t\"DATA\": {\n",
    "\t\t\"height\": 224,\n",
    "\t\t\"width\": 224,\n",
    "\t\t\"batch_size\": 10,\n",
    "\t\t\"norm_mean\": [\n",
    "\t\t\t0.485,\n",
    "\t\t\t0.456,\n",
    "\t\t\t0.406\n",
    "\t\t],\n",
    "\t\t\"norm_std\": [\n",
    "\t\t\t0.229,\n",
    "\t\t\t0.224,\n",
    "\t\t\t0.225\n",
    "\t\t],\n",
    "\t\t\"source\": source_folder,\n",
    "\t\t\"target\": target_folder\n",
    "\t},\n",
    "\t\"MODEL\": {\n",
    "\t\t\"arch\": \"resnet18\",\n",
    "        \"n_layer_feature\" : [\n",
    "            \"maxpool\",\n",
    "            \"layer1.1.relu_1\",\n",
    "            \"layer2.1.relu_1\", \n",
    "            \"layer3.1.relu_1\", \n",
    "            \"layer4.1.relu_1\"],\n",
    "        \"feature_extractors_layers_weights\" : [1, 1, 1, 1, 1],\n",
    "        \"device\": \"cpu\"\n",
    "\t},\n",
    "\t\"METHOD\": {\n",
    "\t\t\"name\": \"cmd\",\n",
    "        \"k\": 5\n",
    "\t}\n",
    "}\n",
    "\n",
    "\n",
    "# Compute the metric\n",
    "dist = cmd.compute(cmd_config_json)\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(f\"metric name: {cmd_config_json['METHOD'][\"name\"]}\")\n",
    "print(f\"source folder: {source_folder} ({len(os.listdir(source_folder))} images)\")\n",
    "print(f\"target folder: {target_folder} ({len(os.listdir(target_folder))} images)\")\n",
    "print(\"Preprocessing:\")\n",
    "print(\" \"*4+f\"-image resize: ({cmd_config_json[\"DATA\"][\"width\"]},{cmd_config_json[\"DATA\"][\"height\"]})\")\n",
    "print(\" \"*4+\"-image normalize:\")\n",
    "print(\" \"*8+f\"-mean: {cmd_config_json[\"DATA\"][\"norm_mean\"]}\")\n",
    "print(\" \"*8+f\"-std: {cmd_config_json[\"DATA\"][\"norm_std\"]}\")\n",
    "print(f\"device: {cmd_config_json[\"MODEL\"][\"device\"]}\")\n",
    "print(f\"model architecture: {cmd_config_json[\"MODEL\"][\"arch\"]}\")\n",
    "print(f\"feature extraction layer: {cmd_config_json[\"MODEL\"][\"n_layer_feature\"]}\")\n",
    "print(f\"Central Moments Discrepancy score: {dist}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# remove generated source and target folder\n",
    "# !rm -r synthetic_source_images/ synthetic_target_images/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(source_folder)))\n",
    "print(len(os.listdir(target_folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yoann/data/synthetic_source_images_mini\n"
     ]
    }
   ],
   "source": [
    "print(source_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "6650915840.0\n",
    "105973.578125\n",
    "\n",
    "2573.7099609375"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
